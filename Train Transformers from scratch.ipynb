{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Uncomment and run this cell if you're on Colab or Kaggle\n# !git clone https://github.com/nlp-with-transformers/notebooks.git\n# %cd notebooks\n# from install import *\n# install_requirements(is_chapter10=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:20:42.391560Z","iopub.execute_input":"2024-02-29T11:20:42.392312Z","iopub.status.idle":"2024-02-29T11:20:42.414615Z","shell.execute_reply.started":"2024-02-29T11:20:42.392278Z","shell.execute_reply":"2024-02-29T11:20:42.413485Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfrom transformers import pipeline, set_seed\n\ngeneration_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\ngeneration_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:20:45.043323Z","iopub.execute_input":"2024-02-29T11:20:45.043690Z","iopub.status.idle":"2024-02-29T11:21:57.327820Z","shell.execute_reply.started":"2024-02-29T11:20:45.043661Z","shell.execute_reply":"2024-02-29T11:21:57.326566Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-02-29 11:20:53.388290: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-29 11:20:53.388423: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-29 11:20:53.547541: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"031413fcfea441c9a737cd6f6b0adf11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f0b5b0a0b441eab5bfba7d11a4b543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c007f02f18644307b9d1a92655372a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3052c4b81f7840a4999067690eb822e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e699b4ac2148f49d0eb8e6433c9766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd25fa3d8e9c4e65a9d750bb5f0706d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb25d6a897949faa205cc9660342c0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9543fcf895b4b15807626f3f9a6b104"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4093687efe7c4e6e9a3403f7387be078"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94817e2a0cbc40e9b9a3a8d167506e6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea4578f4fb84dfbb5580a296b4ae7bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0088b2057a4452387984f9f688f23eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9617b57acacc48fea4103f49fd4509b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a434a85e7d0a446fb26606985011abe0"}},"metadata":{}}]},{"cell_type":"code","source":"def model_size(model):\n    return sum(t.numel() for t in model.parameters())\n\nprint(f\"GPT  size: {model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\nprint(f\"GPT2 size: {model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:22:02.267291Z","iopub.execute_input":"2024-02-29T11:22:02.268312Z","iopub.status.idle":"2024-02-29T11:22:02.279403Z","shell.execute_reply.started":"2024-02-29T11:22:02.268263Z","shell.execute_reply":"2024-02-29T11:22:02.277872Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPT  size: 116.5M parameters\nGPT2 size: 124.4M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"set_seed(1)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:22:06.397601Z","iopub.execute_input":"2024-02-29T11:22:06.398000Z","iopub.status.idle":"2024-02-29T11:22:06.408037Z","shell.execute_reply.started":"2024-02-29T11:22:06.397969Z","shell.execute_reply":"2024-02-29T11:22:06.406919Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def enum_pipeline_ouputs(pipe, prompt, num_return_sequences):\n    out = pipe(prompt, num_return_sequences=num_return_sequences,\n               clean_up_tokenization_spaces=True)\n    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n\nprompt = \"\\nWhen they came back\"\nprint(\"GPT completions:\\n\" + enum_pipeline_ouputs(generation_gpt, prompt, 3))\nprint(\"\")\nprint(\"GPT-2 completions:\\n\" + enum_pipeline_ouputs(generation_gpt2, prompt, 3))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:22:09.963427Z","iopub.execute_input":"2024-02-29T11:22:09.963855Z","iopub.status.idle":"2024-02-29T11:22:25.472404Z","shell.execute_reply.started":"2024-02-29T11:22:09.963824Z","shell.execute_reply":"2024-02-29T11:22:25.470693Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"GPT completions:\n1.\nWhen they came back, she 'd want to know what he was going to do. \n \" it 'll teach your brother a lesson, \" he said, and then he looked pointedly at my arm and the blood he 'd poured over my skin. \"\n2.\nWhen they came back, a little boy with a black nose and brown eyes who had started hanging around the neighborhood on special occasions and who lived off of the property. he wore a football jersey with the letters \" the black diamond \" written on it.\n3.\nWhen they came back, and they were still there. \" \n jake nodded. \n i shrugged and sat back down. \" so then, they did come back, \" i said, thinking. \" but... \" \n \" what's her name?\n\nGPT-2 completions:\n1.\nWhen they came back the body was still on fire, and there were a lot of people who had died. The smell of diesel had been getting worse. No one was doing well. The streets were deserted, with no water and no running water\n2.\nWhen they came back to the door, they came back with the fire extinguisher.\n\nThe man, who was carrying only a small gun, shot at the gas and the man's car. As soon as the gas came out of the\n3.\nWhen they came back to the ship, Raine had a choice of waiting or risking an extra ship while he and his crew kept him on the island. Raine got the choice of waiting or having the opportunity to go see the island where he\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### 流式访问","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset, DownloadConfig \n\nstreamed_dataset = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:32:55.347159Z","iopub.execute_input":"2024-02-29T11:32:55.347579Z","iopub.status.idle":"2024-02-29T11:32:57.658683Z","shell.execute_reply.started":"2024-02-29T11:32:55.347546Z","shell.execute_reply":"2024-02-29T11:32:57.657513Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f023cda8cbb4b52a9155f9f086e5a84"}},"metadata":{}}]},{"cell_type":"code","source":"iterator = iter(streamed_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:33:27.918055Z","iopub.execute_input":"2024-02-29T11:33:27.918449Z","iopub.status.idle":"2024-02-29T11:33:27.923435Z","shell.execute_reply.started":"2024-02-29T11:33:27.918417Z","shell.execute_reply":"2024-02-29T11:33:27.922229Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"remote_dataset = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:34:26.932603Z","iopub.execute_input":"2024-02-29T11:34:26.933037Z","iopub.status.idle":"2024-02-29T11:34:28.707635Z","shell.execute_reply.started":"2024-02-29T11:34:26.933005Z","shell.execute_reply":"2024-02-29T11:34:28.706567Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# hide_output\nfrom transformers import AutoTokenizer\n\ndef tok_list(tokenizer, string):\n    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n    return [tokenizer.decode(tok) for tok in input_ids]\n\ntokenizer_T5 = AutoTokenizer.from_pretrained(\"t5-base\")\ntokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:37:36.865903Z","iopub.execute_input":"2024-02-29T11:37:36.866338Z","iopub.status.idle":"2024-02-29T11:37:46.160667Z","shell.execute_reply.started":"2024-02-29T11:37:36.866307Z","shell.execute_reply":"2024-02-29T11:37:46.159645Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50925cd729e94f71ba00c096fe987a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c00927413b4396b31913ca93b89bf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ada3f87226274f4ca68e5ac8b530c3d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4690e44233ef430a92e3dbe384654ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba6d37af67c474aa30bce6a88595723"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48261f1278e54458bd9ffc5cad883b32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e614c44d53ca43fa97bdad2904b5bbe6"}},"metadata":{}}]},{"cell_type":"code","source":"print(f'T5 tokens for \"sex\": {tok_list(tokenizer_T5,\"sex\")}')\nprint(f'CamemBERT tokens for \"being\": {tok_list(tokenizer_camembert,\"being\")}')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:37:48.339032Z","iopub.execute_input":"2024-02-29T11:37:48.339413Z","iopub.status.idle":"2024-02-29T11:37:48.346242Z","shell.execute_reply.started":"2024-02-29T11:37:48.339384Z","shell.execute_reply":"2024-02-29T11:37:48.345104Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"T5 tokens for \"sex\": ['', 's', 'ex']\nCamemBERT tokens for \"being\": ['be', 'ing']\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\npython_code = r\"\"\"def say_hello():\n    print(\"Hello, World!\")\n# Print it\nsay_hello()\n\"\"\"\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\nprint(tokenizer(python_code).tokens())\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:38:13.451664Z","iopub.execute_input":"2024-02-29T11:38:13.452248Z","iopub.status.idle":"2024-02-29T11:38:13.821747Z","shell.execute_reply.started":"2024-02-29T11:38:13.452209Z","shell.execute_reply":"2024-02-29T11:38:13.820589Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer.backend_tokenizer.normalizer)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:38:28.151277Z","iopub.execute_input":"2024-02-29T11:38:28.151712Z","iopub.status.idle":"2024-02-29T11:38:28.157798Z","shell.execute_reply.started":"2024-02-29T11:38:28.151676Z","shell.execute_reply":"2024-02-29T11:38:28.156571Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:38:38.934625Z","iopub.execute_input":"2024-02-29T11:38:38.935062Z","iopub.status.idle":"2024-02-29T11:38:38.941570Z","shell.execute_reply.started":"2024-02-29T11:38:38.935029Z","shell.execute_reply":"2024-02-29T11:38:38.940383Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello', (28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n","output_type":"stream"}]},{"cell_type":"code","source":"a, e = u\"a\", u\"€\"\nbyte = ord(a.encode(\"utf-8\"))\nprint(f'`{a}` is encoded as `{a.encode(\"utf-8\")}` with a single byte: {byte}')\nbyte = [ord(chr(i)) for i in e.encode(\"utf-8\")]\nprint(f'`{e}` is encoded as `{e.encode(\"utf-8\")}` with three bytes: {byte}')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:38:48.607108Z","iopub.execute_input":"2024-02-29T11:38:48.607532Z","iopub.status.idle":"2024-02-29T11:38:48.615045Z","shell.execute_reply.started":"2024-02-29T11:38:48.607500Z","shell.execute_reply":"2024-02-29T11:38:48.613849Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"`a` is encoded as `b'a'` with a single byte: 97\n`€` is encoded as `b'\\xe2\\x82\\xac'` with three bytes: [226, 130, 172]\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n\nbyte_to_unicode_map = bytes_to_unicode()\nunicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\nbase_vocab = list(unicode_to_byte_map.keys())\n\nprint(f'Size of our base vocabulary: {len(base_vocab)}')\nprint(f'First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:38:51.361098Z","iopub.execute_input":"2024-02-29T11:38:51.361524Z","iopub.status.idle":"2024-02-29T11:38:51.369245Z","shell.execute_reply.started":"2024-02-29T11:38:51.361488Z","shell.execute_reply":"2024-02-29T11:38:51.368343Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Size of our base vocabulary: 256\nFirst element: `!`, last element: `Ń`\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport pandas as pd\nfrom transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n\nbyte_to_unicode_map = bytes_to_unicode()\nunicode_to_byte_map = dict((v, k) for k, v in byte_to_unicode_map.items())\nbase_vocab = list(unicode_to_byte_map.keys())\n\nexamples = [\n    ['Regular characters', '`a` and `?`', f'{ord(\"a\")} and {ord(\"?\")}' , f'`{byte_to_unicode_map[ord(\"a\")]}` and `{byte_to_unicode_map[ord(\"?\")]}`'],\n    ['Nonprintable control character (carriage return)', '`U+000D`', f'13', f'`{byte_to_unicode_map[13]}`'],\n    ['A space', '` `', f'{ord(\" \")}', f'`{byte_to_unicode_map[ord(\" \")]}`'],\n    ['A nonbreakable space', '`\\\\xa0`', '160', f'`{byte_to_unicode_map[ord(chr(160))]}`'],\n    ['A newline character', '`\\\\n`', '10', f'`{byte_to_unicode_map[ord(chr(10))]}`'],\n]\n\npd.DataFrame(examples, columns = ['Description', 'Character', 'Bytes', 'Mapped bytes'])","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:38:56.976723Z","iopub.execute_input":"2024-02-29T11:38:56.977126Z","iopub.status.idle":"2024-02-29T11:38:57.004501Z","shell.execute_reply.started":"2024-02-29T11:38:56.977097Z","shell.execute_reply":"2024-02-29T11:38:57.003567Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                        Description    Character      Bytes  \\\n0                                Regular characters  `a` and `?`  97 and 63   \n1  Nonprintable control character (carriage return)     `U+000D`         13   \n2                                           A space          ` `         32   \n3                              A nonbreakable space       `\\xa0`        160   \n4                               A newline character         `\\n`         10   \n\n  Mapped bytes  \n0  `a` and `?`  \n1          `č`  \n2          `Ġ`  \n3          `ł`  \n4          `Ċ`  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Character</th>\n      <th>Bytes</th>\n      <th>Mapped bytes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Regular characters</td>\n      <td>`a` and `?`</td>\n      <td>97 and 63</td>\n      <td>`a` and `?`</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Nonprintable control character (carriage return)</td>\n      <td>`U+000D`</td>\n      <td>13</td>\n      <td>`č`</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A space</td>\n      <td>` `</td>\n      <td>32</td>\n      <td>`Ġ`</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A nonbreakable space</td>\n      <td>`\\xa0`</td>\n      <td>160</td>\n      <td>`ł`</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A newline character</td>\n      <td>`\\n`</td>\n      <td>10</td>\n      <td>`Ċ`</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:39:03.608347Z","iopub.execute_input":"2024-02-29T11:39:03.608733Z","iopub.status.idle":"2024-02-29T11:39:03.614321Z","shell.execute_reply.started":"2024-02-29T11:39:03.608703Z","shell.execute_reply":"2024-02-29T11:39:03.613267Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('():', (13, 16)), ('ĊĠĠĠ', (16, 20)), ('Ġprint', (20, 26)), ('(\"', (26, 28)), ('Hello', (28, 33)), (',', (33, 34)), ('ĠWorld', (34, 40)), ('!\")', (40, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Size of the vocabulary: {len(tokenizer)}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:39:05.874822Z","iopub.execute_input":"2024-02-29T11:39:05.875282Z","iopub.status.idle":"2024-02-29T11:39:05.889834Z","shell.execute_reply.started":"2024-02-29T11:39:05.875249Z","shell.execute_reply":"2024-02-29T11:39:05.888600Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Size of the vocabulary: 50257\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer(python_code).tokens())","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:39:08.758696Z","iopub.execute_input":"2024-02-29T11:39:08.759091Z","iopub.status.idle":"2024-02-29T11:39:08.765626Z","shell.execute_reply.started":"2024-02-29T11:39:08.759062Z","shell.execute_reply":"2024-02-29T11:39:08.764273Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"['def', 'Ġsay', '_', 'hello', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 训练一个标记器","metadata":{}},{"cell_type":"code","source":"tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\nprint([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[:8]])\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:05.867843Z","iopub.execute_input":"2024-02-29T11:40:05.868231Z","iopub.status.idle":"2024-02-29T11:40:05.940318Z","shell.execute_reply.started":"2024-02-29T11:40:05.868202Z","shell.execute_reply":"2024-02-29T11:40:05.938969Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', ' =================================================================', ' ----------------------------------------------------------------', '________________________________________________________________', '================================================================', '----------------------------------------------------------------', 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', '................................................................']\n","output_type":"stream"}]},{"cell_type":"code","source":"tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\nprint([f'{tokenizer.convert_tokens_to_string([t])}' for t, _ in tokens[:12]]);","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:53.003866Z","iopub.execute_input":"2024-02-29T11:40:53.004259Z","iopub.status.idle":"2024-02-29T11:40:53.079280Z","shell.execute_reply.started":"2024-02-29T11:40:53.004231Z","shell.execute_reply":"2024-02-29T11:40:53.078027Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated', ' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nlength = 100000\ndataset_name = 'codeparrot/github-code'\ndataset = load_dataset(dataset_name, split=\"train\", streaming=True)\niter_dataset = iter(dataset)\n\n\n# Adjust the key based on the actual structure of the dictionaries returned by iter_dataset\ndef batch_iterator(batch_size=10, content_key='code'):\n    for _ in tqdm(range(0, length, batch_size)):\n        yield [next(iter_dataset)[content_key] for _ in range(batch_size)]\n\n# Use the adjusted batch_iterator with the correct content key\nnew_tokenizer = tokenizer.train_new_from_iterator(\n    batch_iterator(content_key='code'),  # Use the correct content key\n    vocab_size=12500,\n    initial_alphabet=base_vocab\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:54:29.522229Z","iopub.execute_input":"2024-02-29T11:54:29.522624Z","iopub.status.idle":"2024-02-29T12:05:03.443807Z","shell.execute_reply.started":"2024-02-29T11:54:29.522595Z","shell.execute_reply":"2024-02-29T12:05:03.442015Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db9e7ff152784a59bea77c06d052cc7f"}},"metadata":{}},{"name":"stdout","text":"\n\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m [\u001b[38;5;28mnext\u001b[39m(iter_dataset)[content_key] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Use the adjusted batch_iterator with the correct content key\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m new_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_new_from_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the correct content key\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_alphabet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_vocab\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:791\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.train_new_from_iterator\u001b[0;34m(self, text_iterator, vocab_size, length, new_special_tokens, special_tokens_map, **kwargs)\u001b[0m\n\u001b[1;32m    789\u001b[0m trainer_class \u001b[38;5;241m=\u001b[39m MODEL_TO_TRAINER_MAPPING[tokenizer_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    790\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainer_class(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, special_tokens\u001b[38;5;241m=\u001b[39mspecial_tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 791\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_from_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m post_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m     trained_tokenizer_json \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(tokenizer\u001b[38;5;241m.\u001b[39mto_str())\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\nprint([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[257:280]]);","metadata":{"execution":{"iopub.status.busy":"2024-02-29T12:05:03.446334Z","iopub.status.idle":"2024-02-29T12:05:03.446707Z","shell.execute_reply.started":"2024-02-29T12:05:03.446522Z","shell.execute_reply":"2024-02-29T12:05:03.446538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print([f'{new_tokenizer.convert_tokens_to_string(t)}' for t,_ in tokens[-12:]]);","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"[' capt', ' embedded', ' regarding', 'Bundle', '355', ' recv', ' dmp', ' vault',\n\n' Mongo', ' possibly', 'implementation', 'Matches']\n"}]},{"cell_type":"code","source":"print(new_tokenizer(python_code).tokens())","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"['def', 'Ġs', 'ay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n\n'ĠWor', 'ld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 's', 'ay', '_', 'hello',\n\n'()', 'Ċ']\n"}]},{"cell_type":"code","source":"import keyword\n\nprint(f'There are in total {len(keyword.kwlist)} Python keywords.')\nfor keyw in keyword.kwlist:\n    if keyw not in new_tokenizer.vocab:\n        print(f'No, keyword `{keyw}` is not in the vocabulary')","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"There are in total 35 Python keywords.\n\nNo, keyword `await` is not in the vocabulary\n\nNo, keyword `finally` is not in the vocabulary\n\nNo, keyword `nonlocal` is not in the vocabulary\n"}]},{"cell_type":"code","source":"# hide_output\nlength = 200000\nnew_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(),\n    vocab_size=32768, initial_alphabet=base_vocab)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"100%|██████████| 200/200 [05:08<00:00,  1.54s/it]\n"}]},{"cell_type":"code","source":"tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1],\n                reverse=False)\nprint([f'{tokenizer.convert_tokens_to_string(t)}' for t, _ in tokens[-12:]]);","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"['lineEdit', 'spik', ' BC', 'pective', 'OTA', 'theus', 'FLUSH', ' excutils',\n\n'00000002', ' DIVISION', 'CursorPosition', ' InfoBar']\n"}]},{"cell_type":"code","source":"print(new_tokenizer_larger(python_code).tokens())","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n\n'ĠWorld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_', 'hello', '()',\n\n'Ċ']\n"}]},{"cell_type":"code","source":"for keyw in keyword.kwlist:\n    if keyw not in new_tokenizer_larger.vocab:\n        print(f'No, keyword `{keyw}` is not in the vocabulary')","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"No, keyword `nonlocal` is not in the vocabulary\n"}]},{"cell_type":"markdown","source":"### Saving a Custom Tokenizer on the Hub","metadata":{}},{"cell_type":"code","source":"#hide_output\nmodel_ckpt = \"codeparrot\"\norg = \"transformersbook\"\nnew_tokenizer_larger.push_to_hub(model_ckpt, organization=org)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Cloning https://huggingface.co/transformersbook/codeparrot into local empty directory.\n"},{"execution_count":null,"output_type":"execute_result","data":{"text/plain":["'https://huggingface.co/transformersbook/codeparrot/commit/1c284adaa3cc9f8635ae7e3377bd3739f48bc09a'"]},"metadata":{}}]},{"cell_type":"code","source":"reloaded_tokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\nprint(reloaded_tokenizer(python_code).tokens())","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"['def', 'Ġsay', '_', 'hello', '():', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',',\n\n'ĠWorld', '!\")', 'Ġ#', 'ĠPrint', 'Ġit', 'Ċ', 'Ċ', 'say', '_', 'hello', '()',\n\n'Ċ']\n"}]},{"cell_type":"code","source":"#hide_output\nnew_tokenizer.push_to_hub(model_ckpt+ \"-small-vocabulary\", organization=org)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Cloning https://huggingface.co/transformersbook/codeparrot-small-vocabulary into local empty directory.\n"},{"execution_count":null,"output_type":"execute_result","data":{"text/plain":["'https://huggingface.co/transformersbook/codeparrot-small-vocabulary/commit/0b37bed9956d95d0b79ada169f6a281e15c63381'"]},"metadata":{}}]},{"cell_type":"code","source":"#hide_output\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(org + \"/\" + model_ckpt)\nconfig = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\nmodel = AutoModelForCausalLM.from_config(config)","metadata":{},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be84ca77ca144954af8ae4820ec6685b","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/787 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","source":"print(f'GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters')","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"GPT-2 (xl) size: 1529.6M parameters\n"}]},{"cell_type":"code","source":"#hide_output\nmodel.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True,\n                      organization=org)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nconfig_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\nmodel_small = AutoModelForCausalLM.from_config(config_small)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters')","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"GPT-2 size: 111.0M parameters\n"}]},{"cell_type":"code","source":"#hide_output\nmodel_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True,\n                            organization=org)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Cloning https://huggingface.co/transformersbook/codeparrot-small into local empty directory.\n"}]},{"cell_type":"code","source":"#hide_output\nexamples, total_characters, total_tokens = 500, 0, 0\ndataset = load_dataset('transformersbook/codeparrot-train', split='train',\n                       streaming=True)\n\nfor _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n    total_characters += len(example['content'])\n    total_tokens += len(tokenizer(example['content']).tokens())\n\ncharacters_per_token = total_characters / total_tokens","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"  0%|          | 1/500 [00:00<01:16,  6.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2605 > 1024). Running this sequence through the model will result in indexing errors\n\n100%|██████████| 500/500 [00:04<00:00, 122.59it/s]\n"}]},{"cell_type":"code","source":"print(characters_per_token)","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"3.6233025034779565\n"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import IterableDataset\n\nclass ConstantLengthDataset(IterableDataset):\n    \n    def __init__(self, tokenizer, dataset, seq_length=1024,\n                 num_of_sequences=1024, chars_per_token=3.6):\n        self.tokenizer = tokenizer\n        self.concat_token_id = tokenizer.eos_token_id\n        self.dataset = dataset\n        self.seq_length = seq_length\n        self.input_characters = seq_length * chars_per_token * num_of_sequences\n    \n    def __iter__(self):\n        iterator = iter(self.dataset)\n        more_examples = True\n        while more_examples:\n            buffer, buffer_len = [], 0\n            while True:\n                if buffer_len >= self.input_characters:\n                    m=f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n                    print(m)\n                    break\n                try:\n                    m=f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n                    print(m)\n                    buffer.append(next(iterator)[\"content\"])\n                    buffer_len += len(buffer[-1])\n                except StopIteration:\n                    iterator = iter(self.dataset)\n\n            all_token_ids = []\n            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n            for tokenized_input in tokenized_inputs['input_ids']:\n                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n            \n            for i in range(0, len(all_token_ids), self.seq_length):\n                input_ids = all_token_ids[i : i + self.seq_length]\n                if len(input_ids) == self.seq_length:\n                    yield torch.tensor(input_ids)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shuffled_dataset = dataset.shuffle(buffer_size=100)\nconstant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset,\n                                                num_of_sequences=10)\ndataset_iterator = iter(constant_length_dataset)\n\nlengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\nprint(f\"Lengths of the sequences: {lengths}\")","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Fill buffer: 0<36864\n\nFill buffer: 3311<36864\n\nFill buffer: 9590<36864\n\nFill buffer: 22177<36864\n\nFill buffer: 25530<36864\n\nFill buffer: 31098<36864\n\nFill buffer: 32232<36864\n\nFill buffer: 33867<36864\n\nBuffer full: 41172>=36864\n\nLengths of the sequences: [1024, 1024, 1024, 1024, 1024]\n"}]},{"cell_type":"code","source":"from argparse import Namespace\n\n# Commented parameters correspond to the small model\nconfig = {\"train_batch_size\": 2, # 12\n          \"valid_batch_size\": 2, # 12\n          \"weight_decay\": 0.1,\n          \"shuffle_buffer\": 1000,\n          \"learning_rate\": 2e-4, # 5e-4\n          \"lr_scheduler_type\": \"cosine\",\n          \"num_warmup_steps\": 750, # 2000\n          \"gradient_accumulation_steps\": 16, # 1\n          \"max_train_steps\": 50000, # 150000\n          \"max_eval_steps\": -1,\n          \"seq_length\": 1024,\n          \"seed\": 1,\n          \"save_checkpoint_steps\": 50000} # 15000\n\nargs = Namespace(**config)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nimport logging\nimport wandb\n\ndef setup_logging(project_name):\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, handlers=[\n        logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n        logging.StreamHandler()])\n    if accelerator.is_main_process: # We only want to set up logging once\n        wandb.init(project=project_name, config=args)\n        run_name = wandb.run.name\n        tb_writer = SummaryWriter()\n        tb_writer.add_hparams(vars(args), {'0': 0})\n        logger.setLevel(logging.INFO)\n        datasets.utils.logging.set_verbosity_debug()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        tb_writer = None\n        run_name = ''\n        logger.setLevel(logging.ERROR)\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n    return logger, tb_writer, run_name","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_metrics(step, metrics):\n    logger.info(f\"Step {step}: {metrics}\")\n    if accelerator.is_main_process:\n        wandb.log(metrics)\n        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfrom torch.utils.data.dataloader import DataLoader\n\ndef create_dataloaders(dataset_name):\n    train_data = load_dataset(dataset_name+'-train', split=\"train\",\n                              streaming=True)\n    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,\n                                    seed=args.seed)\n    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\",\n                              streaming=True)\n    \n    train_dataset = ConstantLengthDataset(tokenizer, train_data,\n                                          seq_length=args.seq_length)\n    valid_dataset = ConstantLengthDataset(tokenizer, valid_data,\n                                          seq_length=args.seq_length)\n    \n    train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)\n    eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n    return train_dataloader, eval_dataloader","metadata":{},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"328dc6d7d05c452e8d8e2cab5b4b9c4e","version_major":2,"version_minor":0},"text/plain":["Check remote data files:   0%|          | 0/183 [00:00<?, ?it/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"Using custom data configuration codeparrot-train-938ce362e6f661b1\n\nUsing custom data configuration codeparrot-valid-29167601d8e69487\n"}]},{"cell_type":"code","source":"def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n    params_with_wd, params_without_wd = [], []\n    for n, p in model.named_parameters():\n        if any(nd in n for nd in no_decay):\n            params_without_wd.append(p)\n        else:\n            params_with_wd.append(p)\n    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n            {'params': params_without_wd, 'weight_decay': 0.0}]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate():\n    model.eval()\n    losses = []\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(batch, labels=batch)\n        loss = outputs.loss.repeat(args.valid_batch_size)\n        losses.append(accelerator.gather(loss))\n        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n    loss = torch.mean(torch.cat(losses))\n    try:\n\t\tperplexity = torch.exp(loss)\n\texcept OverflowError:\n\t\tperplexity = torch.tensor(float(\"inf\"))\n    return loss.item(), perplexity.item()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(args.seed)\n\n# Accelerator\naccelerator = Accelerator()\nsamples_per_step = accelerator.state.num_processes * args.train_batch_size\n\n# Logging\nlogger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\nlogger.info(accelerator.state)\n\n# Load model and tokenizer\nif accelerator.is_main_process:\n    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\nmodel = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\ntokenizer = AutoTokenizer.from_pretrained(\"./\")\n\n# Load dataset and dataloader\ntrain_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n\n# Prepare the optimizer and learning rate scheduler\noptimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\nlr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,\n                             num_warmup_steps=args.num_warmup_steps,\n                             num_training_steps=args.max_train_steps,)\ndef get_lr():\n    return optimizer.param_groups[0]['lr']\n\n# Prepare everything with our `accelerator` (order of args is not important)\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader)\n\n# Train model\nmodel.train()\ncompleted_steps = 0\nfor step, batch in enumerate(train_dataloader, start=1):\n    loss = model(batch, labels=batch).loss\n    log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,\n                       'steps': completed_steps, 'loss/train': loss.item()})\n    loss = loss / args.gradient_accumulation_steps\n    accelerator.backward(loss)\n    if step % args.gradient_accumulation_steps == 0:\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        completed_steps += 1\n    if step % args.save_checkpoint_steps == 0:\n        logger.info('Evaluating and saving model checkpoint')\n        eval_loss, perplexity = evaluate()\n        log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        if accelerator.is_main_process:\n            unwrapped_model.save_pretrained(\"./\")\n            hf_repo.push_to_hub(commit_message=f'step {step}')\n        model.train()\n    if completed_steps >= args.max_train_steps:\n        break\n\n# Evaluate and save the last checkpoint\nlogger.info('Evaluating and saving model after training')\neval_loss, perplexity = evaluate()\nlog_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})\naccelerator.wait_for_everyone()\nunwrapped_model = accelerator.unwrap_model(model)\nif accelerator.is_main_process:\n    unwrapped_model.save_pretrained(\"./\")\n    hf_repo.push_to_hub(commit_message=f'final model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#hide_output\nfrom transformers import pipeline, set_seed\n\nmodel_ckpt = 'transformersbook/codeparrot-small'\ngeneration = pipeline('text-generation', model=model_ckpt, device=0)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"2021-10-20 18:29:01.107727: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n\n2021-10-20 18:29:01.107759: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"}]},{"cell_type":"code","source":"import re\nfrom transformers import set_seed \n\ndef first_block(string):\n    return re.split('\\nclass|\\ndef|\\n#|\\n@|\\nprint|\\nif', string)[0].rstrip()\n\ndef complete_code(pipe, prompt, max_length=64, num_completions=4, seed=1):\n    set_seed(seed)\n    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1,\n                  \"do_sample\":True,}\n    code_gens = generation(prompt, num_return_sequences=num_completions, \n                            max_length=max_length, **gen_kwargs)\n    code_strings = []\n    for code_gen in code_gens:\n        generated_code = first_block(code_gen['generated_text'][len(prompt):])\n        code_strings.append(generated_code)\n    print(('\\n'+'='*80 + '\\n').join(code_strings))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = '''def area_of_rectangle(a: float, b: float):\n    \"\"\"Return the area of the rectangle.\"\"\"'''\ncomplete_code(generation, prompt)","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\n    return math.sqrt(a * b)\n\n================================================================================\n\n\n\n    return a * b / 2.0\n\n================================================================================\n\n\n\n    return a * b\n\n================================================================================\n\n\n\n    return a * b / a\n"}]},{"cell_type":"code","source":"prompt = '''def get_urls_from_html(html):\n    \"\"\"Get all embedded URLs in a HTML string.\"\"\"'''\ncomplete_code(generation, prompt)","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\n    if not html:\n\n        return []\n\n    return [url for url in re.findall(r'<a href=\"(/[^/]+/[^\"]+?)\">', html)]\n\n================================================================================\n\n\n\n    return [url for url in re.findall(r'<a href=\"(.*?)\"', html)\n\n            if url]\n\n================================================================================\n\n\n\n    return [url for url in re.findall(r'<a href=\"(/.*)\",', html)]\n\n================================================================================\n\n\n\n    return re.findall(r'<a href=\"(.*?)\" class=\"url\"[^>]*>', html)\n"}]},{"cell_type":"code","source":"import requests\n\ndef get_urls_from_html(html):\n    return [url for url in re.findall(r'<a href=\"(.*?)\"', html) if url]\n\nprint(\" | \".join(get_urls_from_html(requests.get('https://hf.co/').text)))","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"https://github.com/huggingface/transformers | /allenai | /facebook |\n\n/asteroid-team | /google | /amazon | /speechbrain | /microsoft | /grammarly |\n\n/models | /inference-api | /distilbert-base-uncased |\n\n/dbmdz/bert-large-cased-finetuned-conll03-english |\n\nhttps://huggingface.co/transformers | https://arxiv.org/abs/1811.06031 |\n\nhttps://arxiv.org/abs/1803.10631 | https://transformer.huggingface.co/ | /coref\n\n| https://medium.com/huggingface/distilbert-8cf3380435b5\n"}]},{"cell_type":"code","source":"model_ckpt = 'transformersbook/codeparrot'\ngeneration = pipeline('text-generation', model=model_ckpt, device=0)\n\nprompt = '''# a function in native python:\ndef mean(a):\n    return sum(a)/len(a)\n\n# the same function using numpy:\nimport numpy as np\ndef mean(a):'''\ncomplete_code(generation, prompt, max_length=64)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"},{"name":"stdout","output_type":"stream","text":"\n\n    return np.mean(a)\n\n================================================================================\n\n\n\n    return np.mean(a)\n\n================================================================================\n\n\n\n    return np.mean(a)\n\n================================================================================\n\n\n\n    return np.mean(a)\n"}]},{"cell_type":"code","source":"prompt = '''X = np.random.randn(100, 100)\ny = np.random.randint(0, 1, 100)\n\n# fit random forest classifier with 20 estimators'''\ncomplete_code(generation, prompt, max_length=96)","metadata":{},"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":"Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"},{"name":"stdout","output_type":"stream","text":"\n\nreg = DummyRegressor()\n\n\n\nforest = RandomForestClassifier(n_estimators=20)\n\n\n\nforest.fit(X, y)\n\n================================================================================\n\n\n\nclf = ExtraTreesClassifier(n_estimators=100, max_features='sqrt')\n\nclf.fit(X, y)\n\n================================================================================\n\n\n\nclf = RandomForestClassifier(n_estimators=20, n_jobs=n_jobs, random_state=1)\n\nclf.fit(X, y)\n\n================================================================================\n\n\n\nclf = RandomForestClassifier(n_estimators=20)\n\nclf.fit(X, y)\n"}]}]}